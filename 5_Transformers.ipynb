{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student: Rodolfo Lerma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention and Transformers   \n",
    "\n",
    "## Machine Learning 530\n",
    "\n",
    "## Steve Elston"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction   \n",
    "\n",
    "This lab will give you some hands on experience training an English to Spanish translation model using transformers. This is a simple example of a sequence-to-sequence model used in natural language processing (NLP). \n",
    "\n",
    "Before continuing, you will need to establish a Google Colab account. If you do not have an account you can sign up on [this page](https://colab.research.google.com/signup). A free account will be adequate for this lab. \n",
    "\n",
    "------------------\n",
    "> **Note:** If you do not wish to run the notebook in Colab, you can download the example notebook and run it in another environment. Depending on the environment, doing so may require minor changes in the notebooks. \n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting and running the notebook\n",
    "\n",
    "You will now start and execute a Jupyter notebook containing a Keras transformer example. Go to [this page](https://keras.io/examples/nlp/neural_machine_translation_with_transformer/), and click **View in Colab**. Once the notebook launches in Colab read the provided commentary and examine the code for each cell. Then, execute the code all the cells in order. The model training runs for at least 30 epochs with batch size of 1302, which will take some time.       \n",
    "\n",
    "> **Note:** For this assignment, you are required to submit this notebook with the exercises complete along with the executed notebook from Colab. To submit your executed notebook you must use File -> Download -> Download .ipynb. You can then upload your notebook into Canvas.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 5-1:** Examine the code used to instantiate the `TransformerEncoder` class and provide short answers to the following questions: \n",
    "> 1. In terms of the three types of attention models, which one is used as a layer in the encoder and why? **Hint:** Examine the code for the `call` method. \n",
    "> 2. How many heads does the transformer layer in the encoder use?    \n",
    "> 3. If you wanted to increase the model capacity, how would you change this hyperparameter? \n",
    "> 4. What will this change in hyperparameter affect amount of training data required, the computing time required and the chance of over-fitting the model? \n",
    "> 5. How is the dimension of the encoding transformer layer key vector specified, why is the the correct choice, and what is the value? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**   \n",
    "> 1. As mentioned in class we have three types of attention models: self attention in encoder, masked self attention for decoder output & cross attention in decoder. In this case it seems we are using self attention.     \n",
    "> 2. This particular instantiation is using 8 heads.       \n",
    "> 3. We would increase the number of heads similarly as what we would do for a CNN by increasing the number of filters in the convolution layer.      \n",
    "> 4. The change in the amount of data will be directly impacted as each head is a different filter than has its own weights to be learn, also the computing time will increase and assuming we do not get more data we have bigger chances of overfitting. See comparison table below:\n",
    "\n",
    "| Number of Heads | Trainable Params | Time per Epoch (AVG) |\n",
    "| -- | -- | -- |\n",
    "| 8 | 19,960,216 | 90s |\n",
    "| 16 | 26,270,104 | 120s |\n",
    "\n",
    "> 5.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 5-2:** Examine the code used to instantiate the `PositionalEmbedding` class and provide short answers to the following questions:\n",
    "> 1. Does the encoder layer use positional embedding? **Hint:** To find out, look at the code used and summary produced for training the model.  \n",
    "> 2. Why is positional encoding a correct choice, considering the structure of the queries, for this application? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**   \n",
    "> 1. Yes   \n",
    "\n",
    "positional_embedding (Position  (None, None, 256)   3845120 ['encoder_inputs[0][0]']alEmbedding)  \n",
    "\n",
    "> 2. As discussed in the book \"self-attention\" is a set-processing mechanism, focused on the relationships between pairs of sequence elements, but it is blind to whether these elements occur at the beginning, middle or the end of a sequence. Transformers are a hybrid approach that is technically order-agnostic, but manually injects order/sequencing information into the process by using positional encodings.\n",
    "This adds the word's positions in the sentence to each word embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 5-3:** Examine the code used to instantiate the `TransformerDecoder` class and provide short answers to the following questions:  \n",
    "> 1. In terms of the three types of attention models, which one is used for the `attention_1` layer in the encoder and why? **Hint:** Examine the code for the `call` method.   \n",
    "> 2. Why is the choice of attention model you noted for the answer to the previous question appropriate for this layer? \n",
    "> 3. In terms of the three types of attention models, which one is used for the `attention_2` layer in the encoder and why? **Hint:** Examine the code for the `call` method.  \n",
    "> 4. Why is the choice of attention model you noted for the answer to the previous question appropriate for this layer? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**    \n",
    "> 1. Masked self attention for decoder output (attention_mask=causal_mask)   \n",
    "> 2.      \n",
    "> 3. attention_mask=padding_mask    \n",
    "> 4.         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright 2022, Stephen F Elston. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
