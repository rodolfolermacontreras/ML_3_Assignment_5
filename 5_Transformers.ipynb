{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23a64dc5",
   "metadata": {},
   "source": [
    "## Student: Rodolfo Lerma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699e9ea6",
   "metadata": {},
   "source": [
    "# Attention and Transformers   \n",
    "\n",
    "## Machine Learning 530\n",
    "\n",
    "## Steve Elston"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802cec2e",
   "metadata": {},
   "source": [
    "## Introduction   \n",
    "\n",
    "This lab will give you some hands on experience training an English to Spanish translation model using transformers. This is a simple example of a sequence-to-sequence model used in natural language processing (NLP). \n",
    "\n",
    "Before continuing, you will need to establish a Google Colab account. If you do not have an account you can sign up on [this page](https://colab.research.google.com/signup). A free account will be adequate for this lab. \n",
    "\n",
    "------------------\n",
    "> **Note:** If you do not wish to run the notebook in Colab, you can download the example notebook and run it in another environment. Depending on the environment, doing so may require minor changes in the notebooks. \n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e6223a",
   "metadata": {},
   "source": [
    "## Starting and running the notebook\n",
    "\n",
    "You will now start and execute a Jupyter notebook containing a Keras transformer example. Go to [this page](https://keras.io/examples/nlp/neural_machine_translation_with_transformer/), and click **View in Colab**. Once the notebook launches in Colab read the provided commentary and examine the code for each cell. Then, execute the code all the cells in order. The model training runs for at least 30 epochs with batch size of 1302, which will take some time.       \n",
    "\n",
    "> **Note:** For this assignment, you are required to submit this notebook with the exercises complete along with the executed notebook from Colab. To submit your executed notebook you must use File -> Download -> Download .ipynb. You can then upload your notebook into Canvas.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a0d4cd",
   "metadata": {},
   "source": [
    "> **Exercise 5-1:** Examine the code used to instantiate the `TransformerEncoder` class and provide short answers to the following questions: \n",
    "> 1. In terms of the three types of attention models, which one is used as a layer in the encoder and why? **Hint:** Examine the code for the `call` method. \n",
    "> 2. How many heads does the transformer layer in the encoder use?    \n",
    "> 3. If you wanted to increase the model capacity, how would you change this hyperparameter? \n",
    "> 4. What will this change in hyperparameter affect amount of training data required, the computing time required and the chance of over-fitting the model? \n",
    "> 5. How is the dimension of the encoding transformer layer key vector specified, why is the the correct choice, and what is the value? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd08cb3",
   "metadata": {},
   "source": [
    "> **Answers:**   \n",
    "> 1. As mentioned in class we have three types of attention models: self attention in encoder, masked self attention for decoder output & cross attention in decoder. In this case it seems we are using self attention.     \n",
    "> 2. This particular instantiation is using 8 heads.       \n",
    "> 3. We would increase the number of heads similarly as what we would do for a CNN by increasing the number of filters in the convolution layer.      \n",
    "> 4. The change in the amount of data will be directly impacted as each head is a different filter than has its own weights to be learn, also the computing time will increase and assuming we do not get more data we have bigger chances of overfitting. See comparison table below:\n",
    "\n",
    "| Number of Heads | Trainable Params | Time per Epoch (AVG) |\n",
    "| -- | -- | -- |\n",
    "| 8 | 19,960,216 | 90s |\n",
    "| 16 | 26,270,104 | 120s |\n",
    "\n",
    "> 5. It is specified from the `embed_dim` which in this example is passed with a value **256**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70b3bf0",
   "metadata": {},
   "source": [
    "> **Exercise 5-2:** Examine the code used to instantiate the `PositionalEmbedding` class and provide short answers to the following questions:\n",
    "> 1. Does the encoder layer use positional embedding? **Hint:** To find out, look at the code used and summary produced for training the model.  \n",
    "> 2. Why is positional encoding a correct choice, considering the structure of the queries, for this application? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9935bb19",
   "metadata": {},
   "source": [
    "> **Answers:**   \n",
    "> 1. Yes   \n",
    "\n",
    "positional_embedding (Position  (None, None, 256)   3845120 ['encoder_inputs[0][0]']alEmbedding)  \n",
    "\n",
    "> 2. As discussed in the book and in class \"self-attention\" is a set-processing mechanism, focused on the relationships between pairs of sequence elements, but it is blind to whether these elements occur at the beginning, middle or the end of a sequence. Transformers are a hybrid approach that is technically order-agnostic, but manually injects order/sequencing information into the process by using positional encodings.\n",
    "This adds the word's positions in the sentence to each word embedding, which is a way to re-inject order information to the model and by doing that potentially improving the context on the text. For this application (Machine Translation) it is important to have the sequence/position would give context information that is important to match at the values level to get a good translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab2d5cb",
   "metadata": {},
   "source": [
    "> **Exercise 5-3:** Examine the code used to instantiate the `TransformerDecoder` class and provide short answers to the following questions:  \n",
    "> 1. In terms of the three types of attention models, which one is used for the `attention_1` layer in the encoder and why? **Hint:** Examine the code for the `call` method.   \n",
    "> 2. Why is the choice of attention model you noted for the answer to the previous question appropriate for this layer? \n",
    "> 3. In terms of the three types of attention models, which one is used for the `attention_2` layer in the encoder and why? **Hint:** Examine the code for the `call` method.  \n",
    "> 4. Why is the choice of attention model you noted for the answer to the previous question appropriate for this layer? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be2d87c",
   "metadata": {},
   "source": [
    "> **Answers:**    \n",
    "> 1. Masked self attention for decoder output is attention_mask=causal_mask.\n",
    "> 2. As explained in class having a masked self attention layer reduces the use of recurrent units. As mentioned in class the TransformerDecoder is order-agnostic and it looks at the entire secuence at once, and if were allowed it will simply copy the sequence then getting 100% accuracy. Then by masking the attention matrix we prevent the model from looking into the future and therefore memorizing the sequences.     \n",
    "> 3. attention_mask=padding_mask    \n",
    "> 4. This makes sense as the input sequence does not have to match the size of the output for a translation application, then adding a masked self attention with padding should help to match any length therefore adding flexibility to the model and still preventing the model from memorizing the sequences.       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29375ab4",
   "metadata": {},
   "source": [
    "#### Copyright 2022, Stephen F Elston. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
